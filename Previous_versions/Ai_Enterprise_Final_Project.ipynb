{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d39872f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Sentiment Analysis\n",
    "https://www.repustate.com/blog/sentiment-analysis-steps/<br>\n",
    "Sentiment analysis is the AI-powered method through which brands can find out the emotions that customers express about them on the internet. \n",
    "## Step0: Data collection\n",
    "## Step1: Preprocessing text data\n",
    "Tokenization -> Texting cleaning / Processing -> Text Vectorization <br><br>\n",
    "Tokenization Methods: 1. NLTK 2.Keras Tokenizer API<br>\n",
    "Text Vectorization Methods: 1. Bag of Words (BOW). 2. One Hot Encoding. 3. Term Frequency, Inverse Term Frequency (TF-IDF, BOW extension). 4. The Word Embedding Model(Pretrained: Word2Vec, GloVe, Keras Embedding Layer).\n",
    "## Step2: Data Analysis\n",
    "Training the model -> multilingual processing -> custom tags -> topic/aspect classification -> sentiment analysis <br><br>\n",
    "Sentiment Analysis: Each aspect and theme is isolated in this stage by the platform and then analysed for the sentiment. Sentiment scores are given in the range of -1 to +1. A neutral statement may be termed as zero. \n",
    "## Step3: Data Visualization (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc8035",
   "metadata": {},
   "source": [
    "![SentimentAnalysisStructure](tableOfContent_W9_note.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23e732",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274437ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Load data & Data exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e6ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86e43f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.compat.v1.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.compat.v1.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d691ac24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('F:/Durham_College_AI/2- semester/AI in enterprise/Final_project/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dbe5d7a-77e6-4f80-9783-13721de3ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = [\"text\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab512875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10387</td>\n",
       "      <td>10387</td>\n",
       "      <td>10361</td>\n",
       "      <td>10387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10374</td>\n",
       "      <td>9816</td>\n",
       "      <td>8482</td>\n",
       "      <td>10374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  title  author   text\n",
       "label                             \n",
       "0      10387  10387   10361  10387\n",
       "1      10374   9816    8482  10374"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check is this a balance dataset\n",
    "data.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0a66c66-fd27-4dc6-8df8-a7e240c7eaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20761"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1227d6c-f155-4240-8d44-9263911ac095",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = data[\"text\"].str.len().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "583f9fc2-8ae8-4217-972d-399edaa957ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.iloc[19764][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61740637",
   "metadata": {},
   "source": [
    "# Insight:\n",
    "positive only has 1/4 amount of data compare to negative tweets, this may affect my model lean to negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27583b",
   "metadata": {},
   "source": [
    "train = 10728 rows * 0.7 = 7509.6 <br>\n",
    "test = 10728 * 0.3 = the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61887ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset into 70% vs 30% based on insight found above\n",
    "X_train = data.loc[:len(data)*0.7, 'text'].values\n",
    "y_train = data.loc[:len(data)*0.7, 'label'].values\n",
    "X_test = data.loc[(len(data)*0.7) + 1:, 'text'].values\n",
    "y_test = data.loc[(len(data)*0.7) + 1:, 'label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb66ec",
   "metadata": {},
   "source": [
    "# Step1:Preprocessing text data\n",
    "<h4>Tokenization (use Keras Tokenizer API)<br>\n",
    "    & Texting cleaning / Processing <br>\n",
    "    & Text Vectorization (use Keras Word Embedding Model)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3035413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6d991",
   "metadata": {},
   "source": [
    "# Tokenization &Texting Cleaning & Vectorization\n",
    "Here I will format the text samples and labels into tensors that can be fed into a neural network.<br>\n",
    "To do this, I will utilize <strong>Keras.preprocessing.text.Tokenizer</strong> and <strong>keras.preprocessing.sequence.pad_sequences</strong>.<br><br>\n",
    "Note: By default, <strong>Keras.preprocessing.text.Tokenizer</strong> removes all punctuation, turns the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bff3692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     7   7543      8 ...  21178  36565   4240]\n",
      " [  7649      5  75613 ...  20764   1169  13568]\n",
      " [    38   1515   3485 ...    670   8657   8883]\n",
      " ...\n",
      " [203023    843      1 ...      9     39    320]\n",
      " [   182      2      1 ...     10  15267   2259]\n",
      " [    24    322    397 ...      1    876     11]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize Tokenizer class\n",
    "tokenizer_obj = Tokenizer()\n",
    "# tockenize all the tweets\n",
    "total_tweets = data.loc[:,'text'].values\n",
    "tokenizer_obj.fit_on_texts(total_tweets) \n",
    "\n",
    "# Keras prefers inputs to be vectorized and all inputs to have the same length\n",
    "# so I need to pad sequences\n",
    "indx = data[\"text\"].str.len().idxmax()\n",
    "max_length = len(data.iloc[19764][\"text\"])\n",
    "#max_length = 50 # based on calculation in csv file (max: 29, average: 17)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer_obj.word_index) + 1\n",
    "# tockenize train and test dataset\n",
    "X_train_tokens =  tokenizer_obj.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer_obj.texts_to_sequences(X_test)\n",
    "# pad sequence tockenized-train and tockenized-test dataset\n",
    "# parameter explanation: padding https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "# String, 'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence. \n",
    "X_train_pad = pad_sequences(X_train_tokens,maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_tokens,maxlen = max_length, padding='post')\n",
    "print(X_train_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7b3bf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14501"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a6bc720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238052\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7597d2b9-97cf-41b0-af17-9f887210d174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6259, 264)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e93ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "there is 15820 number of vocabulary in my dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7abe230",
   "metadata": {},
   "source": [
    "# Step2: Data Analysis - Training the model \n",
    "<h2> LSTM model training </h2>\n",
    "Ready to define my neural network model.<br><br>\n",
    "The model will use an <strong>Embedding layer</strong> as the first hidden layer. The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset during training of the model.<br>\n",
    "Second layer is <strong>LSTM</strong>, then <strong>output layer (classification)</strong><br>\n",
    "LSTM parameters: https://keras.io/api/layers/recurrent_layers/lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b83f285b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building LSTM model with Tensorflow/Keras...\n",
      "Summary of the built model...\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 264, 300)          71415600  \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (None, 264, 32)           42624     \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 264, 32)           8320      \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,477,041\n",
      "Trainable params: 71,477,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# set embedding dimension is 200 (maybe too much for my small dataset, but whatever...)\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "print('Building LSTM model with Tensorflow/Keras...')\n",
    "\n",
    "# initialize Sequential class in order to struture my neural network model\n",
    "model = Sequential()\n",
    "# use keras word embedding layer as my first input layer\n",
    "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length)\n",
    "model.add(embedding_layer)\n",
    "# one layer of LSTM\n",
    "model.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.2,return_sequences = True))\n",
    "\n",
    "model.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.2,return_sequences = True))\n",
    "\n",
    "model.add(LSTM(units=32,  dropout=0.2, recurrent_dropout=0.2,return_sequences = False))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# output layer using sigmoid as activation method (0-1)\n",
    "model.add(Dense(1)) # classification problem: so output = 1)\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print('Summary of the built model...')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc490e",
   "metadata": {},
   "source": [
    "Explanation of the summary above:\n",
    "<li>Embedding layers is 50 words x 200 vector dimension\n",
    "<li>LSTM is 32 dimension of the output space\n",
    "<li>Dense = Final output layer is 1 output only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c4d725ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10\n",
      "114/114 [==============================] - 225s 2s/step - loss: 0.3784 - accuracy: 0.8559 - val_loss: 0.2723 - val_accuracy: 0.8933\n",
      "Epoch 2/10\n",
      "114/114 [==============================] - 230s 2s/step - loss: 0.1778 - accuracy: 0.9462 - val_loss: 0.2731 - val_accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "114/114 [==============================] - 224s 2s/step - loss: 0.0907 - accuracy: 0.9774 - val_loss: 0.2373 - val_accuracy: 0.9289\n",
      "Epoch 4/10\n",
      "114/114 [==============================] - 224s 2s/step - loss: 0.0605 - accuracy: 0.9834 - val_loss: 0.2532 - val_accuracy: 0.9323\n",
      "Epoch 5/10\n",
      "114/114 [==============================] - 228s 2s/step - loss: 0.0527 - accuracy: 0.9841 - val_loss: 0.3520 - val_accuracy: 0.9155\n",
      "Epoch 6/10\n",
      "114/114 [==============================] - 226s 2s/step - loss: 0.0499 - accuracy: 0.9832 - val_loss: 0.3249 - val_accuracy: 0.9121\n",
      "Epoch 7/10\n",
      "114/114 [==============================] - 226s 2s/step - loss: 0.0441 - accuracy: 0.9866 - val_loss: 0.3926 - val_accuracy: 0.9155\n",
      "Epoch 8/10\n",
      "114/114 [==============================] - 225s 2s/step - loss: 0.0547 - accuracy: 0.9835 - val_loss: 0.2792 - val_accuracy: 0.9155\n",
      "Epoch 9/10\n",
      "114/114 [==============================] - 225s 2s/step - loss: 0.0443 - accuracy: 0.9882 - val_loss: 0.2649 - val_accuracy: 0.9278\n",
      "Epoch 10/10\n",
      "114/114 [==============================] - 225s 2s/step - loss: 0.1216 - accuracy: 0.9723 - val_loss: 0.3565 - val_accuracy: 0.8890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25264eeeca0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=10, validation_data=(X_test_pad, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0311ee52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238052, 300)\n"
     ]
    }
   ],
   "source": [
    "# Check weights matrix in the embedding layer\n",
    "print(embedding_layer.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9cd4a4",
   "metadata": {},
   "source": [
    "There is 15820 rows and 200 columns in both embedding matrix and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc576765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00382903 -0.00587053  0.04335074 ...  0.12898205  0.01420071\n",
      "   0.00967517]\n",
      " [ 0.07686709  0.07374097  0.09413031 ... -0.02480163 -0.09206655\n",
      "   0.02909448]\n",
      " [-0.01791714 -0.11168876 -0.06725611 ... -0.03886044  0.09327106\n",
      "  -0.09911796]\n",
      " ...\n",
      " [-0.0281518  -0.04022111 -0.02247751 ... -0.03610227 -0.01409967\n",
      "  -0.03082849]\n",
      " [ 0.02415563 -0.0378341   0.03905306 ... -0.04646444 -0.0249963\n",
      "   0.02757633]\n",
      " [ 0.01512632 -0.02184947  0.02084725 ...  0.01986808 -0.04185306\n",
      "  -0.00685205]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04d8adc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00382903 -0.00587053  0.04335074 -0.0454114   0.02117755  0.03388212\n",
      " -0.04732965 -0.09339476 -0.05080926 -0.04675207  0.05137672  0.18874048\n",
      "  0.08256961 -0.00131278  0.00241474 -0.03182614  0.09045675 -0.13608947\n",
      "  0.04030503 -0.04965901  0.03199952  0.01639097 -0.04001632  0.05339506\n",
      " -0.02994714 -0.05879462 -0.02570869 -0.03116549 -0.04035143 -0.06147698\n",
      " -0.06276058 -0.03623896 -0.00191258 -0.05377633  0.05297501  0.02893719\n",
      " -0.00930509  0.05842903 -0.07084234 -0.01193138 -0.01341305 -0.00723896\n",
      "  0.03643291 -0.00594012  0.01471033 -0.03965194 -0.05313287 -0.07044734\n",
      " -0.01700999 -0.06558758 -0.01982605 -0.05706052  0.03168553  0.01901981\n",
      "  0.07559431 -0.01364453  0.01010045  0.008158   -0.09819352 -0.07177477\n",
      " -0.00759261  0.02659782  0.06549978 -0.03236971  0.01208768  0.04632158\n",
      " -0.05012042  0.0225373  -0.04318896  0.05713596  0.02592877 -0.03936525\n",
      "  0.08377057  0.09214601  0.03102545 -0.08449408 -0.08407471  0.12011185\n",
      " -0.05073658 -0.05673467  0.06373047  0.02344095 -0.0425421   0.01302933\n",
      "  0.06434616 -0.00482796  0.0968232  -0.03424275  0.0156873  -0.0464884\n",
      " -0.00480134  0.03059167 -0.00440225  0.05358343  0.03284059  0.03506289\n",
      " -0.01042415  0.03066321  0.01344571 -0.00745603  0.04380117 -0.04211891\n",
      "  0.05278193  0.0340142  -0.05675942 -0.00541487  0.00772001 -0.01503797\n",
      "  0.03022221 -0.01586295  0.04947298  0.02357939 -0.03736041  0.02923694\n",
      "  0.04475526  0.03749683 -0.05205794  0.05774194  0.01496751 -0.01531969\n",
      " -0.10431143  0.05276272 -0.03809712 -0.04641313 -0.07625685 -0.06050685\n",
      " -0.01426568 -0.06457994 -0.04662953  0.06505924 -0.04118799  0.01828169\n",
      "  0.05680143  0.0349892   0.03095632  0.01839689  0.00323109  0.01944119\n",
      " -0.01579843 -0.05576161  0.05582502 -0.03344358 -0.02708418  0.06919454\n",
      " -0.06214939 -0.00736302  0.029426   -0.042042    0.04904033 -0.01279037\n",
      "  0.03996943 -0.04720829  0.02711002  0.11049516 -0.03685674  0.1027085\n",
      "  0.09703854  0.05302226 -0.02964339  0.08706535 -0.04456936 -0.09203012\n",
      " -0.02924254  0.03675659 -0.02389603  0.08137117 -0.04032043 -0.06085688\n",
      " -0.05379257 -0.09953819 -0.04964151  0.03030116 -0.11771847 -0.00908746\n",
      "  0.03437377 -0.01116606  0.02073013  0.07764047 -0.03203572  0.00389788\n",
      "  0.02730917  0.0073871  -0.03237321 -0.04596153 -0.00395871  0.02229956\n",
      " -0.06871164 -0.00414122 -0.09456262 -0.06402766  0.06986924  0.02157694\n",
      "  0.01411661  0.12032989  0.00084889  0.0654689  -0.01451476  0.12898205\n",
      "  0.01420071  0.00967517]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.get_weights()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee89ddcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "49/49 [==============================] - 13s 268ms/step - loss: 0.3565 - accuracy: 0.8890\n",
      "Test score: 0.3565372824668884\n",
      "Test accuracy: 0.8889598846435547\n",
      "Accuracy: 88.90%\n"
     ]
    }
   ],
   "source": [
    "# check how model fits test dataset\n",
    "print('Testing...')\n",
    "score, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "\n",
    "print(\"Accuracy: {0:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343260a",
   "metadata": {},
   "source": [
    "# compare 70% training dataset with 50% training dataset\n",
    "70% out of total as the training dataset accuracy: 81.85%-83.43% <Br>\n",
    "50% out of total as the training dataset accuracy: 82.55%, it's test score is overfit (1.05%)<br>\n",
    "Therefore, I will use 70% as my training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bce6128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = \"You are bad.\" # should be closer to 0 -> negative tweet\n",
    "# test_sample = \"You are good.\" # should be closer to 1 -> positive tweet\n",
    "# test_sample = \"An update on FoxNews tech failures for the #GOPDebate \"# should be closer to 0 -> negative tweet\n",
    "# test_sample = \"Before the #GOPDebate, 14 focus groupers said they had favorable view of Trump.\"# should be closer to 1 -> positive tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7be686d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_padding(sample):\n",
    "    tokenizer_test = Tokenizer()\n",
    "    test_tweets = sample\n",
    "    tokenizer_test.fit_on_texts(test_tweets) \n",
    "    # padding sequences\n",
    "    max_length = 264 # based on calculation in csv file (max: 29, average: 17)\n",
    "    # define vocabulary size\n",
    "    vocab_size = len(tokenizer_test.word_index) + 1\n",
    "    test_sample_tokens =  tokenizer_obj.texts_to_sequences(test_tweets)\n",
    "    test_samples_tokens_pad = pad_sequences(test_sample_tokens, maxlen=max_length, padding='post')\n",
    "    return test_samples_tokens_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "30c3c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_tokens_pad = tokenized_padding(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "866a603b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358357],\n",
       "       [0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358357],\n",
       "       [0.04358363],\n",
       "       [0.04358363],\n",
       "       [0.04358363]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "model_list = model.predict(x=test_samples_tokens_pad)\n",
    "model_list\n",
    "\n",
    "# The other way to show model_list: seperate array to show the result, otherwise print(model_list) shows ugly e-04 kind of number\n",
    "# for val in model_list:\n",
    "#     print(val)\n",
    "# print(\"Average of the prediction = \", model_list.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e8c92",
   "metadata": {},
   "source": [
    "**Value closer to 1 is strong positive sentiment<br>\n",
    "Value close to 0 is a strong negative sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f18bb6",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430119ce",
   "metadata": {},
   "source": [
    "# test sample = \"You are bad.\" \n",
    "1. should be closer to 0 -> negative tweet<br>\n",
    "2. result:<br>\n",
    "![testSample_Bad](predict_YouAreBad_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb4848",
   "metadata": {},
   "source": [
    "# test sample = \"You are good.\" \n",
    "1. should be closer to 1 -> positive tweet<br>\n",
    "2. result:<br>\n",
    "![testSample_Good](predict_YouAreGood_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f8bca",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7de95d",
   "metadata": {},
   "source": [
    "I've tried NLTK & Keras preprocessing, pre & post padding, 100 & 200 embedding dimension, 35 & 50 maximum vocabulary length, 50% & 70% of data as training set.<br>\n",
    "Based on the performance (accuracy and training time), the best combination of above parameters is post, 200, 50, 70%, respectively. So I suggest to use this combination.<br>\n",
    "Regarding preprocessing methods, NLTK has more steps to manual code it, Keras provides a convenient way to do preprocessing but it will leave ' alone instead of remove it, which is more reliable and readable.<br>\n",
    "If there is a chance, I would like to compare Word2Vec and GloVe with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e9acb",
   "metadata": {},
   "source": [
    "# Generate requirement.txt\n",
    "https://pypi.org/project/pigar/\n",
    "<ol>\n",
    "    <li>Open the terminal under this environment - Anaconda > choose this environment > launch CMD.exe Prompt</li>\n",
    "    <li>navigate to this folder - cd C://xx/xx...</li>\n",
    "    <li>type command - pip install pigar</li>\n",
    "    <li>type command - pigar</li>\n",
    "</ol>\n",
    "In the folder, you will see a \"requirement.txt\" file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
